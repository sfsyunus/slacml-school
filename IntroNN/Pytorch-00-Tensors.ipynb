{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "\n",
    "You can open this notebook either within a supported container or Google colaboratory [here](https://colab.research.google.com/github/slaclab/slacml-school/blob/master/IntroNN/Pytorch-00-Tensors.ipynb).\n",
    "\n",
    "[Pytorch](https://pytorch.org/) is one of open-source, modern deep learning libraries out there and what we will use in this workshop. Other popular libraries include [Tensorflow](https://www.tensorflow.org/), [Keras](https://keras.io), [MXNet](https://mxnet.apache.org), [Spark ML](https://spark.apache.org/mllib/), etc. ...\n",
    "\n",
    "All of those libraries works very similar. If you are new, probably any of Pytorch/Keras/Tensorflow would work well with lots of guidance/examples/discussion-forums online! Common things you have to learn include:\n",
    "\n",
    "1. Array data types (_tensor_ )\n",
    "2. Data loading tools (streamline prepping data into appropraite types from input files)\n",
    "3. Implementation of a ML model\n",
    "\n",
    "In this notebook, we cover the basics of the first item.\n",
    "\n",
    "<a href=\"datatype\"></a>\n",
    "## 1. Tensor data types in PyTorch\n",
    "In `pytorch`, we use `torch.Tensor` object to represent a data array. It is a lot like `numpy` array but not quite the same. `torch` provide APIs to easily convert data between `numpy` array and `torch.Tensor`. Let's play a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f35c785fa50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import torch\n",
    "SEED=123\n",
    "np.random.seed(SEED)    # Setting the seed for reproducibility\n",
    "torch.manual_seed(SEED) # This is how you do for torch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... yep, that's how we set pytorch random number seed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a torch.Tensor\n",
    "\n",
    "Pytorch provides constructors similar to numpy (and named same way where possible to avoid users having to look-up function names). Here are some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.zeros:\n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "torch.ones:\n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "torch.arange:\n",
      " tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]])\n",
      "\n",
      "torch.randn:\n",
      " tensor([[-0.1115,  0.1204, -0.3696],\n",
      "        [-0.2404, -1.1969,  0.2093]])\n"
     ]
    }
   ],
   "source": [
    "# Tensor of 0s = numpy.zeros\n",
    "t=torch.zeros(2,3)\n",
    "print('torch.zeros:\\n',t)\n",
    "\n",
    "# Tensor of 1s = numpy.ones\n",
    "t=torch.ones(2,3)\n",
    "print('\\ntorch.ones:\\n',t)\n",
    "\n",
    "# Tensor from a sequential integers = numpy.arange\n",
    "t=torch.arange(0,6,1).reshape(2,3).float()\n",
    "print('\\ntorch.arange:\\n',t)\n",
    "\n",
    "# Normal distribution centered at 0.0 and sigma=1.0 = numpy.rand.randn\n",
    "t=torch.randn(2,3)\n",
    "print('\\ntorch.randn:\\n',t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or you can create from a simple list, tuple, and numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy data\n",
      " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "\n",
      "torch.Tensor data\n",
      " tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n",
      "\n",
      "Python list : [1, 2, 3]\n",
      "torch.Tensor: tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# Create numpy array\n",
    "data_np = np.zeros([10,10],dtype=np.float32)\n",
    "# Fill something\n",
    "np.fill_diagonal(data_np,1.)\n",
    "print('Numpy data\\n',data_np)\n",
    "\n",
    "# Create torch.Tensor\n",
    "data_torch = torch.Tensor(data_np)\n",
    "print('\\ntorch.Tensor data\\n',data_torch)\n",
    "\n",
    "# One can make also from a list\n",
    "data_list = [1,2,3]\n",
    "data_list_torch = torch.Tensor(data_list)\n",
    "print('\\nPython list :',data_list)\n",
    "print('torch.Tensor:',data_list_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting back from `torch.Tensor` to a numpy array can be easily done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numpy data (converted back from torch.Tensor)\n",
      " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Bringing back into numpy array\n",
    "data_np = data_torch.numpy()\n",
    "print('\\nNumpy data (converted back from torch.Tensor)\\n',data_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordinary operations to an array also exists like `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean tensor(0.1000) std tensor(0.3015) sum tensor(10.)\n"
     ]
    }
   ],
   "source": [
    "# mean & std\n",
    "print('mean',data_torch.mean(),'std',data_torch.std(),'sum',data_torch.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the return of those functions (`mean`,`std`,`sum`) are tensor objects. If you would like a single scalar value, you can call `item` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean 0.10000000149011612 std 0.30151134729385376 sum 10.0\n"
     ]
    }
   ],
   "source": [
    "# mean & std\n",
    "print('mean',data_torch.mean().item(),'std',data_torch.std().item(),'sum',data_torch.sum().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor addition and multiplication\n",
    "Common operations include element-wise multiplication, matrix multiplication, and reshaping. Read the [documentation](https://pytorch.org/docs/stable/tensors.html) to find the right function for what you want to do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two numpy matrices\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "[[1. 1. 1.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]] \n",
      "\n",
      "torch.Tensor element-wise multiplication:\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "torch.Tensor matrix multiplication:\n",
      "tensor([[1., 1., 1.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "torch.Tensor matrix addition:\n",
      "tensor([[ 0., -1., -1.],\n",
      "        [ 0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.]])\n",
      "\n",
      "adding a scalar 1:\n",
      "tensor([[2., 1., 1.],\n",
      "        [1., 2., 1.],\n",
      "        [1., 1., 2.]])\n"
     ]
    }
   ],
   "source": [
    "# Two matrices \n",
    "data_a = np.zeros([3,3],dtype=np.float32)\n",
    "data_b = np.zeros([3,3],dtype=np.float32)\n",
    "np.fill_diagonal(data_a,1.)\n",
    "data_b[0,:]=1.\n",
    "# print them\n",
    "print('Two numpy matrices')\n",
    "print(data_a)\n",
    "print(data_b,'\\n')\n",
    "\n",
    "# Make torch.Tensor\n",
    "torch_a = torch.Tensor(data_a)\n",
    "torch_b = torch.Tensor(data_b)\n",
    "\n",
    "print('torch.Tensor element-wise multiplication:')\n",
    "print(torch_a*torch_b)\n",
    "\n",
    "print('\\ntorch.Tensor matrix multiplication:')\n",
    "print(torch_a.matmul(torch_b))\n",
    "\n",
    "print('\\ntorch.Tensor matrix addition:')\n",
    "print(torch_a-torch_b)\n",
    "\n",
    "print('\\nadding a scalar 1:')\n",
    "print(torch_a+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping\n",
    "\n",
    "You can access the tensor shape via `.shape` attribute like numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch_a shape: torch.Size([3, 3])\n",
      "The 0th dimension size: 3\n"
     ]
    }
   ],
   "source": [
    "print('torch_a shape:',torch_a.shape)\n",
    "print('The 0th dimension size:',torch_a.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, there is a `reshape` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_a.reshape(1,9).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and you can also use -1 in the same way you used for numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_a.reshape(-1,3).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing (Slicing)\n",
    "\n",
    "We can use a similar indexing trick like we tried with a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_a[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or a boolean mask generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True],\n",
       "        [ True, False,  True],\n",
       "        [ True,  True, False]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch_a == 0.\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and slicing with it using `masked_select` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_a[mask]#.masked_select(~mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU acceleration\n",
    "Putting `torch.Tensor` on GPU is as easy as calling `.cuda()` function (and if you want to bring it back to cpu, call `.cpu()` on a `cuda.Tensor`). Let's do a simple speed comparison. \n",
    "\n",
    "Create two arrays with an identical data type, shape, and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 1000x1000 matrix\n",
    "data_np=np.zeros([1000,1000],dtype=np.float32)\n",
    "data_cpu = torch.Tensor(data_np).cpu()\n",
    "data_gpu = torch.Tensor(data_np).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time fifth power of the matrix on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "634 µs ± 16.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "mean = (data_cpu ** 5).mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and next on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.6 µs ± 601 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "mean = (data_gpu ** 5).mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... which is more than x10 faster than the cpu counter part :)\n",
    "\n",
    "But there's a catch you should be aware! Preparing a data on GPU does take time because data needs to be sent to GPU, which could take some time. Let's compare the time it takes to create a tensor on CPU v.s. GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.8 µs ± 5.17 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "data_np=np.zeros([1000,1000],dtype=np.float32)\n",
    "data_cpu = torch.Tensor(data_np).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.03 ms ± 92.6 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "data_np=np.zeros([1000,1000],dtype=np.float32)\n",
    "data_gpu = torch.Tensor(data_np).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it takes nearly 10 times longer time to create this particular data tensor on our GPU. This speed depends on many factors including your hardware configuration (e.g. CPU-GPU communication via PCI-e or NVLINK). It makes sense to move computation that takes longer than this data transfer time to perform on GPU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
