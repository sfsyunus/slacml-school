{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression + PyTorch\n",
    "\n",
    "You can open this notebook either within a supported container or Google colaboratory [here](https://colab.research.google.com/github/slaclab/slacml-school/blob/master/IntroNN/Pytorch-01-LinearModels.ipynb).\n",
    "\n",
    "In this notebook, we will introduce ourself to PyTorch machine learning framework by implement a linear regression model. Then we expand into a model with multiple linear transformation in the form of a simplest neural network. We exercise using a non-linear activation function and how that can impact the solution found by a model. \n",
    "\n",
    "### What we cover\n",
    "1. Logistic regression with a linear model using PyTorch\n",
    "2. Logistic regression with two layers perceptron\n",
    "\n",
    "You may have to install `Tensorboard` (needed if you are running a container w/o Tensorboard) and `plotly>5.3` (needed if you are on google Colab). If so, please execute the following comment cell as a code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! pip3 install --user tensorboard \"plotly>5.3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a regular import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "layout = go.Layout(margin=dict(l=20,r=20,b=20,t=20),\n",
    "                   template='plotly_dark',\n",
    "                   width=500,height=400,\n",
    "                  )\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic regression: red v.s. blue\n",
    "\n",
    "We start with PyTorch library by solving a logistic regression.\n",
    "\n",
    "Our challenge is to separate the red and blue dots using data instances with 2 input features.\n",
    "\n",
    "Here's a function for generating a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_red_and_blue_normal(stat=2000,seed=123):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    data  = np.random.normal(1,1,stat*2).reshape(-1,2)\n",
    "    label = np.zeros(stat).reshape(-1,1)\n",
    "\n",
    "    mask = np.arange(stat)\n",
    "    np.random.shuffle(mask)\n",
    "    mask = mask[0:int(stat/2)]\n",
    "    \n",
    "    data [mask] -= 2\n",
    "    label[mask]  = 1 \n",
    "    \n",
    "    return torch.Tensor(data),torch.Tensor(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and always visualize to check it's not crazy :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "data,label = generate_red_and_blue_normal()\n",
    "\n",
    "blue=data[(label[:,0]==0)]\n",
    "red =data[(label[:,0]==1)]\n",
    "\n",
    "traces = [go.Scatter(x=blue[:,0],y=blue[:,1],opacity=0.7,mode='markers',name='blue'),\n",
    "          go.Scatter(x= red[:,0],y= red[:,1],opacity=0.7,mode='markers',name='red'),\n",
    "         ]\n",
    "fig = go.Figure(traces,layout=layout)\n",
    "fig.update_layout(xaxis_title='p0',yaxis_title='p1')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Building a model\n",
    "\n",
    "1. Define a linear model $\\sigma\\left(\\mathbf{w}\\cdot\\mathbf{x}+b\\right)$ to classify red v.s. blue data points.\n",
    "2. Use gradient descent to optimize the model parameters $\\mathbf{w}$ and $b$ (three parameters).\n",
    "\n",
    "#### 1.1.1 `torch.nn.Linear` module\n",
    "\n",
    "Like many other ML libraries (e.g. `scikit-learn`), Pytorch comes with useful tools to build ML model. In particular, Pytorch provides algorithms in _modules_ and they can be easily combined to build a complex, large models such as a deep neural network. These modules come with capability to compute a derivative with respect to the input as well as parameters of the algorithm to be optimized. This enables gradient-based optimization for a composite model through a chain-rule (i.e. _back propagation_ of gradients).\n",
    "\n",
    "One of the most frequently used is the `torch.nn.Linear` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Linear module that takes 1 input and produces 1 output with a bias term\n",
    "f=torch.nn.Linear(in_features=1,out_features=1,bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module `f` performs a linear transformation:\n",
    "$$ \\text{output} = \\mathbf{w}\\cdot\\mathbf{x} + b$$\n",
    "where $\\mathbf{x}\\in\\mathcal{R}^d$, $\\mathbf{w}\\in\\mathcal{R}^{k\\times d}$, and $b\\in\\mathcal{R}$. In the above construction, $d=1$ and $k=1$ are set by `in_features=1` and `out_features=1` respectively. The argument `bias=True` can be used to enable or remove the bias term in the model. \n",
    "\n",
    "The model parameters are randomly initialized (which we won't discuss here but it's typically sampled from a zero-centered normal distribution with _roughly_ a unit variance). Let's print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f.weight)\n",
    "print(f.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Pytorch modules are designed to take a _batch_ of input data. As such, the input data dimension must be 2 or larger where the first dimension specifies the size of input data. For instance, if we want to compute the model for an input data batch of size $n$ (i.e. $n$ data samples), the input should have the shape $(n,1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy=torch.zeros(5).float().reshape(-1,1)\n",
    "print('Input shape:',toy.shape)\n",
    "print('Output:',f(toy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You almost never need to do this, but if you want to force setting the model parameters, you can do this after disabling the gradient calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    f.weight[0,0]=1.\n",
    "    f.bias[0]=2.\n",
    "    toy=torch.arange(5).float().reshape(-1,1)\n",
    "    print(f(toy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 `torch.nn.Sigmoid`\n",
    "\n",
    "Next, we need a logistic (i.e. sigmoid) function:\n",
    "\n",
    "$$ \\text{Sigmoid}(x) = \\sigma(x) = \\displaystyle{\\frac{1}{1+\\exp(-x)}}$$\n",
    "\n",
    "... which is available as a Pytorch module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=torch.nn.Sigmoid()\n",
    "\n",
    "x = torch.arange(-5,5,0.01)\n",
    "\n",
    "px.scatter(x=x,y=f(x)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 Computation graph\n",
    "\n",
    "Our model, $\\sigma(\\mathbf{w}\\cdot\\mathbf{x}+b)$, can be built by combining two modules (i.e.`torch.nn.Linear` and `torch.nn.Sigmoid`) in a sequence. This form the simplest computation graph: \n",
    "$$\\text{input}\\rightarrow\\text{Linear}\\rightarrow\\text{Sigmoid}\\rightarrow\\text{output}$$\n",
    "\n",
    "How do we build a computation graph instance by combining arbitrary number of operations? \n",
    "\n",
    "There are two typical ways. The first is to use `torch.nn.Sequential` container which can chain arbitrary number of modules to be called in a sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op0=torch.nn.Linear(in_features=2,out_features=1,bias=True)\n",
    "op1=torch.nn.Sigmoid()\n",
    "\n",
    "f = torch.nn.Sequential(op0,op1)\n",
    "print( f(data) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second method is to define your own `torch.nn.Module` inherited class (in which you may use `torch.nn.Sequential` if wished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(model,self).__init__()\n",
    "        self._op0 = torch.nn.Linear(in_features=2, out_features=1, bias=True)\n",
    "        self._op1 = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self._op1( self._op0(x) )\n",
    "\n",
    "f = model()\n",
    "print( f(data) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a way to design our model, let's move onto the optimization!\n",
    "\n",
    "### 1.2 Training our model\n",
    "\n",
    "Recalling from the lecture, below are the steps of gradient-descent optimization:\n",
    "\n",
    "1. Define the model, loss, and optimization algorithm\n",
    "2. Run a training loop\n",
    "    2.1 Compute the loss\n",
    "    2.2 Update model parameters using backpropagated gradients\n",
    "\n",
    "Below is how these steps can be implemented using Pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = torch.nn.BCELoss() # Step 1: Binary Cross-Entropy (BCE) loss\n",
    "optimizer = torch.optim.SGD(params=f.parameters(),lr=0.001) # Step 1: SGD optimizer\n",
    "\n",
    "for _ in range(10000):\n",
    "    \n",
    "    loss = criterion( f(data), label ) # Step 2.1: compute the loss\n",
    "    optimizer.zero_grad() # Step 2.2: reset all gradients to zero\n",
    "    loss.backward()       # Step 2.2: back propagate gradients from the loss\n",
    "    optimizer.step()      # Step 2.2: inform the optimizer about the step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is the model's classification performance? \n",
    "\n",
    "Let's build a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n",
    "import time\n",
    "test_data, test_label = generate_red_and_blue_normal(seed=int(time.time()))\n",
    "\n",
    "def make_confusion_matrix(model,data,label):\n",
    "    pred = f(test_data)\n",
    "    blue = test_label == 0\n",
    "    red  = test_label == 1\n",
    "    tp = (red  & (pred >= 0.5)).sum().item() / red.sum().item()  # true positive\n",
    "    tn = (blue & (pred <  0.5)).sum().item() / blue.sum().item() # true negative\n",
    "    fp = (blue & (pred >= 0.5)).sum().item() / blue.sum().item() # false positive\n",
    "    fn = (red  & (pred <  0.5)).sum().item() / red.sum().item()  # false negative\n",
    "\n",
    "    cm = np.array([[tp, fn],[fp,tn]])\n",
    "    return cm\n",
    "    \n",
    "cm = make_confusion_matrix(f,data,label)\n",
    "cm_txt = cm.astype(np.str)\n",
    "classes=['red','blue']\n",
    "\n",
    "fig = ff.create_annotated_heatmap(z=cm, x=classes, y=classes, annotation_text=cm_txt, colorscale='Viridis')\n",
    "fig.update_layout(width=500,height=500,\n",
    "                  margin=dict(l=20,r=20,t=20,b=20),\n",
    "                  xaxis_title='Prediction',yaxis_title='True label',\n",
    "                 )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the decision boundary of the learned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue=data[(label[:,0]==0)]\n",
    "red =data[(label[:,0]==1)]\n",
    "\n",
    "a,b = f._op0.weight.detach().numpy().reshape(-1)\n",
    "c   = f._op0.bias.item()\n",
    "\n",
    "xedges = np.array([data[:,0].min().item(),data[:,0].max().item()])\n",
    "yedges = (-a/b)*xedges-c/b\n",
    "\n",
    "traces = [go.Scatter(x=blue[:,0],y=blue[:,1],opacity=0.7,mode='markers',name='blue'),\n",
    "          go.Scatter(x= red[:,0],y= red[:,1],opacity=0.7,mode='markers',name='red'),\n",
    "          go.Scatter(x=xedges,y=yedges,name='boundary'),\n",
    "         ]\n",
    "fig = go.Figure(traces,layout=layout)\n",
    "fig.update_layout(xaxis_title='p0',yaxis_title='p1')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Monitoring the training process\n",
    "\n",
    "How can we improve our train procedure? One thing is to monitor the variables during the training. You can do this \"by-hand\" (i.e. writing your own code to keep the log of variable values over iterations), or use a tool like `Tensorboard`. \n",
    "\n",
    "Here is a simple example of `Tensorboard`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "\n",
    "# Create a handler to store the log for Tensorboard\n",
    "writer = SummaryWriter(log_dir='tako')\n",
    "\n",
    "for i in range(5):\n",
    "    t0=time.time()\n",
    "    writer.add_scalar('values/v0',i,i)\n",
    "    writer.add_scalar('values/v1',i**2,i)\n",
    "    writer.add_scalar('values/v2',i**3,i)\n",
    "    writer.add_scalar('time_taken',time.time()-t0,i)\n",
    "\n",
    "writer.add_graph(f,data)\n",
    "writer.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Tensorboard` is most typically used for the purpose of a simple logging and visualization of the logged data. To visualize the log, we can run the `Tensorboard` (either from the command line or in-line within a notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir tako"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's design a `train()` function with `Tensorboard` logging capability. For an analysis to be run later in this notebook, we design this function to also return the model parameter values. You might wonder: \"why not storing model parameter values in the `Tensorboard` and read in later for analysis?\" That is possible, but it does require an extra step of interpretting a binary file written by `Tensorboard`, which we won't cover in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "import time\n",
    "\n",
    "def train(model,\n",
    "          train_data, train_label,\n",
    "          test_data=None, test_label=None,\n",
    "          num_iter=10000,\n",
    "          log_dir=None, store_cycle=100,\n",
    "          optimizer='SGD', lr=0.01):\n",
    "          \n",
    "    if log_dir:\n",
    "        writer = SummaryWriter(log_dir=log_dir)\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optimizer = getattr(torch.optim,optimizer)(model.parameters(),lr=lr)\n",
    "    weights = []\n",
    "    f = IntProgress(min=0,max=int(num_iter/400),bar_style='info')\n",
    "    display(f)\n",
    "    \n",
    "    tstart = time.time()        \n",
    "    for step in range(num_iter):\n",
    "\n",
    "        weights.append(model._op0.weight.detach().numpy().reshape(-1).astype(np.float32))\n",
    "        pred = model(data)\n",
    "        loss = criterion( pred, label )\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        if log_dir and ((step+1)%store_cycle)==0:\n",
    "            with torch.no_grad():\n",
    "                # Monitor model accuracy & loss on the train dataset\n",
    "                accuracy = (((pred < 0.5) & (label < 1)) | ((pred >= 0.5) & (label > 0))).sum().item() / len(pred)\n",
    "                writer.add_scalar('acc/train',accuracy,step)\n",
    "                writer.add_scalar('loss/train',loss,step)\n",
    "                if (step+1)%100 == 0:\n",
    "                    writer.add_histogram('score/train',pred,step)\n",
    "                    \n",
    "                # Monitor model parameters\n",
    "                for i,w in enumerate(model._op0.weight.reshape(-1)):\n",
    "                    writer.add_scalar('weights/p%02d' % i, w, step)\n",
    "                for i,g in enumerate(model._op0.weight.grad.reshape(-1)):\n",
    "                    writer.add_scalar('grads/g%02d' % i, g, step)\n",
    "                    \n",
    "                # Monitor model accuracy & loss on the test dataset (if provided)\n",
    "                if test_data is not None and test_label is not None:\n",
    "                    pred = model(test_data)\n",
    "                    loss = criterion( pred, test_label )\n",
    "                    accuracy = (((pred < 0.5) & (test_label < 1)) | ((pred >= 0.5) & (test_label > 0))).sum().item() / len(pred)\n",
    "                    writer.add_scalar('acc/test',accuracy,step)\n",
    "                    writer.add_scalar('loss/test',loss,step)\n",
    "                    if (step+1)%100 == 0:\n",
    "                        writer.add_histogram('score/test',pred,step)\n",
    "\n",
    "        optimizer.step()      # Step 2.2: inform the optimizer about the step\n",
    "        if step%400 == 0:\n",
    "            f.value +=1\n",
    "            \n",
    "    if log_dir: writer.flush()\n",
    "    print(time.time()-tstart,'[s]')\n",
    "    return np.array(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "f=model()\n",
    "weights=train(f,data,label,test_data,test_label,10000,'aho/run00',100,'SGD')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir aho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Adam optimizer\n",
    "\n",
    "As discussed in the lecture, there are multiple optimizer algorithms available. \n",
    "Let's try running another training with a popular (go-to) choice, `Adam` optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "f=model()\n",
    "weights=train(f,data,label,test_data,test_label,10000,'aho/run01',100,'Adam')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the log directory we just used is under the same directory `aho` as the last training using `SGD`. This allows us to compare two logs side-to-side. Go back to the `Tensorboard` cell and \"reload\". You should see two trainings available for visualization and comparison.\n",
    "\n",
    "We can see `Adam` can flatten the loss curve faster than SGD reaching about the same level of the loss value!\n",
    "\n",
    "Remember a difference depends on data, task, and your model to be optimized. There is no guaranteed \"best solution\". However, in practice, \"Adam\" is a very popular \"default\" choice for neural networks (later).\n",
    "\n",
    "### 1.2.3 More fun\n",
    "\n",
    "Here are two more \"fun\" visualization. The first is the sigmoid score contours overlaid with data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sigmoid_surface(model,data):\n",
    "    rangex = [torch.min(data[:,0]),torch.max(data[:,0])]\n",
    "    rangey = [torch.min(data[:,1]),torch.max(data[:,1])]\n",
    "    # expand the range by +/-20%\n",
    "    rangex[0] -= (rangex[1]-rangex[0])*0.2\n",
    "    rangex[1] += (rangex[1]-rangex[0])*0.2\n",
    "    rangey[0] -= (rangey[1]-rangey[0])*0.2\n",
    "    rangey[1] += (rangey[1]-rangey[0])*0.2\n",
    "    xs = np.arange(rangex[0],rangex[1],(rangex[1]-rangex[0])/100.)\n",
    "    ys = np.arange(rangey[0],rangey[1],(rangey[1]-rangey[0])/100.)\n",
    "    grid = torch.Tensor(np.array(np.meshgrid(xs,ys,sparse=False)).reshape(2,-1))\n",
    "    score = model(torch.swapaxes(grid,0,1)).reshape(len(xs),len(ys)).detach().numpy()\n",
    "    \n",
    "    blue = data[(label[:,0]<1)]\n",
    "    red  = data[(label[:,0]>0)]\n",
    "    \n",
    "    fig = go.Figure(data=[go.Contour(x=xs,y=ys,z=score,name='loss'),\n",
    "                          go.Scatter(x=blue[:,0],y=blue[:,1],opacity=0.7,mode='markers',name='blue'),\n",
    "                          go.Scatter(x= red[:,0],y= red[:,1],opacity=0.7,mode='markers',name='red'),\n",
    "                         ]\n",
    "                   )\n",
    "    fig.update_layout(\n",
    "        autosize=False,\n",
    "        margin=dict(l=20,r=20,b=20,t=20),\n",
    "        width=600,height=400,\n",
    "        xaxis_title='p0',yaxis_title='p1',\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "    \n",
    "plot_sigmoid_surface(f,data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the history of updated parameter values overlaid on top of the loss surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convergence(model,param,data,label):\n",
    "    range0 = [param[:,0].min(),param[:,0].max()]\n",
    "    range1 = [param[:,1].min(),param[:,1].max()]\n",
    "    # expand the range by +/-20%\n",
    "    range0[0] -= (range0[1]-range0[0])*0.2\n",
    "    range0[1] += (range0[1]-range0[0])*0.2\n",
    "    range1[0] -= (range1[1]-range1[0])*0.2\n",
    "    range1[1] += (range1[1]-range1[0])*0.2\n",
    "    w0s = np.arange(range0[0],range0[1],(range0[1]-range0[0])/100.)\n",
    "    w1s = np.arange(range1[0],range1[1],(range1[1]-range1[0])/100.)\n",
    "    grid = torch.Tensor(np.array(np.meshgrid(w0s,w1s,sparse=False)).reshape(2,-1))\n",
    "    \n",
    "    loss_map = np.zeros(shape=(grid.shape[1]))\n",
    "    with torch.no_grad():\n",
    "        criterion = torch.nn.BCELoss()\n",
    "        for i in range(grid.shape[1]):\n",
    "            model._op0.weight[0,0] = grid[0,i]\n",
    "            model._op0.weight[0,1] = grid[1,i]\n",
    "            prediction = model(data)\n",
    "            loss = criterion(prediction, label) \n",
    "            loss_map[i] = loss.mean()\n",
    "    \n",
    "    \n",
    "    loss_map = loss_map.reshape(len(w0s),len(w1s))\n",
    "\n",
    "    fig = go.Figure(data=[go.Contour(x=w0s,y=w1s,z=loss_map),\n",
    "                          go.Scatter(x=param[:,0],y=param[:,1],mode='markers',\n",
    "                                     marker=dict(color=np.log(np.arange(len(param))+1)),name='SGD'),\n",
    "                         ]\n",
    "                   )\n",
    "    fig.update_layout(\n",
    "        autosize=False,\n",
    "        margin=dict(l=20,r=20,b=20,t=20),\n",
    "        width=600,height=400,\n",
    "        xaxis_title='w0',yaxis_title='w1',\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "plot_convergence(f,weights,data,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. Logistic regression with two-layers MLP\n",
    "\n",
    "Let's now make our problem a bit more challenging by modifying our dataset. \n",
    "\n",
    "A new data generator puts red and blue points along straight lines of the same slope but with different offsets.\n",
    "\n",
    "A separation is hence straightforward, except we then re-distribute a half of red points somewhere else.\n",
    "\n",
    "It's probably the simplest to see by eyes. Let's generate the sample and visualize!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_red_and_blue_irregular(a,b,seed=123,distort=False):\n",
    "    np.random.seed(seed)\n",
    "    num_unit=100\n",
    "    x0 = np.zeros(shape=(num_unit*2),dtype=np.float32)\n",
    "    x0[:num_unit] = np.arange(0,3,3.0/num_unit)\n",
    "    x0[num_unit:] = np.arange(2,5,3.0/(num_unit))\n",
    "    x1 = a*x0 + b + np.random.normal(scale=0.3,size=num_unit*2)\n",
    "    x1[:num_unit] += 3\n",
    "    x1[num_unit:] -= 3\n",
    "\n",
    "    y = np.zeros(shape=(num_unit*2),dtype=np.int32)\n",
    "    y[:num_unit] = 0\n",
    "    y[num_unit:] = 1\n",
    "    \n",
    "    # if distort, shift some of label 0\n",
    "    if distort:\n",
    "        idx=np.arange(num_unit)\n",
    "        np.random.shuffle(idx)\n",
    "        idx=idx[:num_unit//2]\n",
    "        x0[idx] = np.random.normal(size=len(idx),scale=0.3)+2.5\n",
    "        x1[idx] = np.random.normal(size=len(idx),scale=0.3)\n",
    "    \n",
    "    \n",
    "    data = np.column_stack([x0,x1,y])\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    return torch.Tensor(data[:,:2]),torch.Tensor(data[:,2].reshape(-1,1))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same data generation function we used before\n",
    "a,b=2.0,4.0\n",
    "data,label=generate_red_and_blue_irregular(a,b,distort=True)\n",
    "blue=data[(label[:,0]==0)]\n",
    "red =data[(label[:,0]==1)]\n",
    "\n",
    "traces = [go.Scatter(x=blue[:,0],y=blue[:,1],opacity=0.7,mode='markers',name='blue'),\n",
    "          go.Scatter(x= red[:,0],y= red[:,1],opacity=0.7,mode='markers',name='red'),\n",
    "         ]\n",
    "fig = go.Figure(traces,layout=layout)\n",
    "fig.update_layout(xaxis_title='p0',yaxis_title='p1')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By just look of it, it is clear we cannot separate two populations cleanly with a straight line. It is intuitively a difficult problem: I imagine different line slopes and offset but not very obvious what works the best immediately. A solution needs to wait till the later part of this notebook :)\n",
    "\n",
    "### Exercise A\n",
    "\n",
    "1. Optimize `model0` for with `Adam` with 10000 steps.\n",
    "2. Visualize the resulting boundary overlaid on the data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, not so great :( \n",
    "\n",
    "### 2.1 Adding another `torch.nn.Linear`\n",
    "\n",
    "During the lecture, we learned about adding a layer may help to linearize a non-linear dataset.\n",
    "\n",
    "Let's build a new model that contains 2 linear layers. This means we have two instances of `torch.nn.Linear`. Make the second instance, which is our linear classifier, to take 2 inputs so we can visualize as a line in 2D space. That means the first instance of `torch.nn.Linear` should produce 2 output features (imagine the first instance contain 2 linear transformations).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(model,self).__init__()\n",
    "        # Define a linear model with a bias term\n",
    "        self._op0  = torch.nn.Linear(2,2,bias=True)\n",
    "        self._op1  = torch.nn.Linear(2,1,bias=True)\n",
    "    def forward(self,x):\n",
    "        return torch.sigmoid(self._op1(self._op0(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise B\n",
    "* Train this new model with the same dataset\n",
    "* Visualize a linear decision boundary like we did before with the input data for the second linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the result did not improve much compared to a single linear transformation. That's because it's hard to distort the location of these data points, which is critical for whether a linear model works well or not, too much with only a linear transformation. \n",
    "\n",
    "That's right... what we have been lacking is a non-linearity.\n",
    "\n",
    "So let's build another model with a non-linear activation function `torch.nn.LeakyReLU`.\n",
    "\n",
    "### 2.3 Adding a non-linearity (Exercise C)\n",
    "\n",
    "1. Design a model with `torch.nn.LeakyReLU` inserted between two linear layers\n",
    "2. Train the model with `Adam` and 10000 iterations\n",
    "3. Visualize the input data and two lines from the first layer\n",
    "4. Visualize the input data and a line from the second layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing\n",
    "\n",
    "By now, hopefully you feel familiar with a logistic regression using a linear model! and also getting used to how to define a model, optimize, and access the output using PyTorch. We will move onto a bit more complex models in the next notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
