{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle image classifier\n",
    "\n",
    "You can open this notebook either within a supported container or Google colaboratory [here](https://colab.research.google.com/github/slaclab/slacml-school/blob/master/IntroNN/Pytorch-05-ParticleImageClassifier.ipynb).\n",
    "\n",
    "In this notebook, we implement a simple CNN to see the effect of data normalization using the dataset for image classification challenge.\n",
    "\n",
    "## Goals\n",
    "* Design and traing a CNN without normalization\n",
    "* Design and train a CNN with batch normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.figsize'] = [8, 6]\n",
    "mpl.rcParams['font.size'] = 16\n",
    "mpl.rcParams['axes.grid'] = True\n",
    "\n",
    "import torch\n",
    "torch.multiprocessing.set_start_method('spawn')\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import numpy as np\n",
    "SEED=12345\n",
    "_=np.random.seed(SEED)\n",
    "_=torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data access\n",
    "\n",
    "* A data file with 400,000 images for training: `train.h5`\n",
    "  * ... which include 100,000 images per particle type\n",
    "* A data file with 100,000 images for testing: `test.h5`\n",
    "  * ... which include 25,000 images per particle type\n",
    "\n",
    "The commands below download the datafile (unless it already exists locally)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ -f \"train.h5\" ]; then\n",
    "    echo \"data file already downloaded\"\n",
    "else \n",
    "    wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1zSQjqn-9tM6rNyuRBvoAoc4YEXFHB0X8' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1zSQjqn-9tM6rNyuRBvoAoc4YEXFHB0X8\" -O test.h5 && rm -rf /tmp/cookies.txt;\n",
    "    wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1V8C51N19u1i7-h2UbDNE1F-t_ffDJlpt' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1V8C51N19u1i7-h2UbDNE1F-t_ffDJlpt\" -O train.h5 && rm -rf /tmp/cookies.txt;\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files are `HDF5` files and can be opened using `h5py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py as h5\n",
    "datapath='./train.h5'\n",
    "\n",
    "# Open a file in 'r'ead mode.\n",
    "f=h5.File(datapath,mode='r',swmr=True) \n",
    "\n",
    "# List items in the file\n",
    "for key in f.keys():\n",
    "    print('dataset',key,'... type',f[key].dtype,'... shape',f[key].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and let's visualize one image for fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry = 0\n",
    "\n",
    "print('PDG code',f['pdg'][entry])\n",
    "plt.imshow(f['image'][entry],origin='lower')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PDG code 13 means muon (if you are unfamiliar, \"PDG code\" is a signed integer as a unique identifier of a particle. See [this documentation](https://pdg.lbl.gov/2006/reviews/pdf-files/montecarlo-web.pdf) for more details.)\n",
    "\n",
    "Let's don't forget to close the file :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Dataset` and `DataLoader`\n",
    "\n",
    "We prepared a simple torch `Dataset` implementation for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ParticleImage2D\n",
    "train_data = ParticleImage2D(data_files=[datapath])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is index-accessible and produce a dictionary with four keys\n",
    "* `data` ... 2D image of a particle (192x192 pixels)\n",
    "* `pdg` ... PDG code of a particle. Should be [11,13,22,2212] = [electron,muon,photon,proton]\n",
    "* `label` ... an integer label for classification\n",
    "* `index` ... an index of the data entry from an input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Size of dataset',len(train_data))\n",
    "\n",
    "# Access the 0-th instance\n",
    "data = train_data[0]\n",
    "\n",
    "# The data instance is a dictionary\n",
    "print('List of keys in a data element',data.keys(),'\\n')\n",
    "\n",
    "# Visualize the image\n",
    "print('Again 0-th image! PDG code %d ... label %d \\n' % (data['pdg'],data['label']))\n",
    "plt.imshow(data['data'],origin='lower')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `DataLoader` instance in a usual way except we give a specifically designed collate function to handle a dictionary style data instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ParticleImage2D(data_files = [datapath], start=0.0, end=0.5)\n",
    "\n",
    "# We use a specifically designed \"collate\" function to create a batch data\n",
    "from utils import collate\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_data,\n",
    "                          collate_fn  = collate,\n",
    "                          shuffle     = True,\n",
    "                          num_workers = 1,\n",
    "                          batch_size  = 64\n",
    "                         )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's measure the speed of the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "tstart=time.time()\n",
    "num_iter=100\n",
    "ctr=num_iter\n",
    "for batch in train_loader:\n",
    "    ctr -=100\n",
    "    if ctr <= 0: break\n",
    "print((time.time()-tstart)/num_iter,'[s/iteration]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN without any normalization\n",
    "Let's design a simple network for particle image classification. We stack convolution layers and x2 downsampling till the image size becomes 192x192 (input size) to 12x12. Then we apply a max-pooling to pick the largest activation over the whole 12x12 per feature. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(CNN, self).__init__()\n",
    "        # feature extractor CNN\n",
    "        self._feature_extractor = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1,32,3,padding=1),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Conv2d(32,32,3,padding=1),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.MaxPool2d(2,2),\n",
    "            \n",
    "            torch.nn.Conv2d(32,64,3,padding=1),\n",
    "            torch.nn.LeakyReLU(),            \n",
    "            torch.nn.Conv2d(64,64,3,padding=1),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.MaxPool2d(2,2),\n",
    "            \n",
    "            torch.nn.Conv2d(64,128,3,padding=1),\n",
    "            torch.nn.LeakyReLU(),            \n",
    "            torch.nn.Conv2d(128,128,3,padding=1),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.MaxPool2d(2,2),\n",
    "            \n",
    "            torch.nn.Conv2d(128,256,3,padding=1),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Conv2d(256,256,3,padding=1),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.MaxPool2d(2,2),\n",
    "            \n",
    "            torch.nn.Conv2d(256,512,3,padding=1),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Conv2d(512,512,3,padding=1),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.MaxPool2d(12,12))\n",
    "        # classifier MLP\n",
    "        self._classifier = torch.nn.Linear(512,4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # extract features\n",
    "        features = self._feature_extractor(x)\n",
    "        # flatten the 3d tensor (2d space x channels = features)\n",
    "        features = features.view(-1, np.prod(features.size()[1:]))\n",
    "        # classifier\n",
    "        return self._classifier(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "\n",
    "def run_train(model, loader,  \n",
    "              num_iterations=100, log_dir='log',\n",
    "              lr=0.001, optimizer='SGD', device=None):\n",
    "    print(\"\\nTraining...\")\n",
    "    tstart = time.time()\n",
    "    if log_dir:\n",
    "        writer = SummaryWriter(log_dir=log_dir)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = getattr(torch.optim,optimizer)(model.parameters(),lr=lr)\n",
    "    f = IntProgress(min=0,max=int(num_iterations/100),bar_style='info')\n",
    "    display(f)\n",
    "    \n",
    "    iteration = 0\n",
    "    while iteration < num_iterations:\n",
    "        for data,label in loader:\n",
    "            \n",
    "            if device:\n",
    "                data,label = data.to(device),label.to(device)\n",
    "\n",
    "            loss = criterion(model(data), label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if log_dir: \n",
    "                writer.add_scalar('loss/train', loss.item(), iteration)\n",
    "            if iteration%100 == 0:\n",
    "                f.value += 1\n",
    "            # Brake if we consumed all iteration counts\n",
    "            iteration += 1\n",
    "            if iteration >= num_iterations:\n",
    "                break\n",
    "    print('done',time.time()-tstart,'[s]')\n",
    "\n",
    "\n",
    "def run_test(model,loader,num_iterations=100,device=None):\n",
    "\n",
    "    label_v, softmax_v = [],[]\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "    f = IntProgress(min=0,max=int(min([len(loader),num_iterations])),bar_style='info')\n",
    "    display(f)\n",
    "    \n",
    "    with torch.set_grad_enabled(False):\n",
    "        for data,label in loader:\n",
    "            if device:\n",
    "                data,label = data.to(device), label.to(device)\n",
    "            label_v.append  ( label.detach().reshape(-1)   )\n",
    "            softmax_v.append( softmax(model(data)).detach())\n",
    "            f.value += 1\n",
    "            num_iterations -= 1\n",
    "            if num_iterations < 1:\n",
    "                break\n",
    "    return torch.concat(label_v).cpu().numpy(), torch.concat(softmax_v).cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network. We use a prepared `train_loop` function from `utils.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "model=CNN().to(device)\n",
    "\n",
    "run_train(model, train_loader, 1000, log_dir='particle_cnn/run00', optimizer='Adam', device=device)\n",
    "\n",
    "torch.save(dict(model = model.state_dict()), 'model-A-1000.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir particle_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance: classification accuracy\n",
    "\n",
    "Let's run the network on a test dataset to see a classification performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For plotting a confusion matrix\n",
    "from utils import plot_confusion_matrix, plot_softmax\n",
    "\n",
    "test_data = ParticleImage2D(data_files=['./test.h5'])\n",
    "test_loader = DataLoader(test_data,\n",
    "                         collate_fn  = collate,\n",
    "                         shuffle     = False,\n",
    "                         num_workers = 2,\n",
    "                         batch_size  = 64,\n",
    "                        )\n",
    "\n",
    "# For the Test set\n",
    "labels, softmax = run_test(model,test_loader,device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy mean',(np.argmax(softmax,axis=1)==labels).astype(np.float).mean(),\n",
    "      'std',(np.argmax(softmax,axis=1)==labels).astype(np.float).std())\n",
    "plot_confusion_matrix(labels,np.argmax(softmax,axis=1),test_data.classes)\n",
    "plot_softmax(labels,softmax,test_data.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving & Restoring the weights\n",
    "\n",
    "Training could take time, and it would be handy to be able to \"store the trained network parameters\" in a file. This can be easily done in any ML libraries including pytorch, and, in fact, we already stored. You should be seeing `model-A-1000.ckpt` in your file browser. \n",
    "\n",
    "Let's exercise how to restore the weights. We first re-create our network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run an inference, this should yield a bad result = accuracy at aroudn 25% (i.e. a random guess among 4 target classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, softmax = run_test(model,test_loader,100,device=device)\n",
    "print('average accuracy',np.mean(labels==np.argmax(softmax,axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the weights using `torch.load` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model-A-1000.ckpt', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    model.load_state_dict(checkpoint['model'], strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run again the inference..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, softmax = run_test(model,test_loader,100,device=device)\n",
    "print('average accuracy',np.mean(labels==np.argmax(softmax,axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Voila!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with batch normalization\n",
    "\n",
    "### Exercise 1\n",
    "Modify the CNN used in the above exercise and add a `Batch Normalization` layer after every convolution layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Train your CNN and compare the loss curve against two loss curves obtained above in this notebook. Run the inference on the test dataset and produce the confusion matrix and softmax score visualization for the newly trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
